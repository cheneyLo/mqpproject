### Meeting Feb 14

This past week we had issues:

- Training variants takes too long!
  - Fewer epochs is okay, we need to track convergence
    - Our NN, by the nature of its data, we won't be as accurate as, say, alzheimer data
  - We can run things in parallel
  - We probably have an issue with vanishing gradients! (vanishing gradients can be a problem in as little as 3 layers.)
    - Should actually train up one layer at a time.
    - With smaller datsets, we should have shallower NNs
- Learning rate calibration issues
  - We're currently doing exponential decay of the learning rate.
  - Standard decay is okay, we can just stop at after a number of iterations
  - Our data are too large to easily overfit...
- Minor coding problems (broke saliency map magick)
- Loosing loss history?
- Ryan doesn't know how to make videos
  - Will just make a bunch of snapshots 1st

##### Next week
- Getting a plot of the learning rates will probably get done 1st
- Deconv, *saliency map* for 3D time series, and other video ones, probably next **highest priority**
- MaxPatch on variants last because they'll take a while to train

By midnight on Saturday, we need need to decide if we're cutting anything.
##### Week after

##### Last half week
