#### What we did
- Got perfect overfitted values on `ace_rsdata_nn`
- Trained one layer at a time, but this didn't give us good loss at all - loss oscillated between low and high
  - This is apparently called pre-training, and is usually used as step 1 of this.
    - Want to patch MaxPatch of each of these layers
  - Training two layers at a time
- Found some code that makes saliency maps much better. (you can change the way backpropagation happens while a layer is running)
  - ("Take an image, go to some layer of the image (usually the output) get a feature and basically do a deconvolutional network")
  - Got 3 different versions of saliency maps
    - Keeps track of zeros going into/out of relu layers (prevents showing viz if backprop would be useless)

- Leave one out visualization
- Basic other graph visualizations

- Miya wrote the sections she worked on & blocked out the rest

#### What we'll do
- Use pre-training as pre-training
  - MaxPatch of this
- Write
  - Note: we want to include all of our work (but perhaps put the less interesting results later)
- Check vizes that are currently rendering
