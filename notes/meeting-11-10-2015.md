# MQP B-term week 3

##### Training large datasets
There is a lot of tricks that we *should* use when traing large datasets to make the training time not too long - Prof. Kong sent us a link a while ago to a course that talks about this.
E.G Start with a large learning rate, and then decrease. Keras *probably* handles this already.

If we need any information about the UMass dataset, we can ask Kong or his contacts there.

### What we did:

* Worked a little on the paper
* Created two NNs (one is a dummy) with the Haxby dataset
    * Visualization for the dummy is done
    * The smart one is currently using convolutional layers over fMRIs with different lengths
* Got *some* code to work on the ACE cluster
    - python2 only, and we're fine with that - just convert the Python3 code over. It shouldn't be much work with 3to2 and the fact we still need to convert from .ipynb
    - An example batch job
* Prof. Kong got the dataset, but we haven't transfered it over to us



### What we will do:

* Run a real batch job on ACE
    - If we're using the real data, we should probably train on ACE

* Create a NN using UMass data - train person recognition (or other labels) data on individual frames.
    - Try to visualize the differences between different layers
    - Maybe predict the values in the summary xlsx (Lac, Ins, Gln... etc)

* Finish up the temporal convolution network
* Start the regional convolution data